{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (!require(\"Metrics\")) install.packages(\"Metrics\")\n",
    "library(\"Metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Models\n",
    "-----------------\n",
    "\n",
    "### Model 1\n",
    "\n",
    "Suppose we have created a model 1 and made some predictions.\n",
    "\n",
    "Here are the actuals from some data and predicted for for that same\n",
    "data:\n",
    "\n",
    "#### Model 1 predictions vs actuals\n",
    "\n",
    "| actuals | predicted |\n",
    "|---------|-----------|\n",
    "| yes     | yes       |\n",
    "| yes     | no        |\n",
    "| yes     | yes       |\n",
    "| yes     | yes       |\n",
    "| no      | yes       |\n",
    "| no      | no        |\n",
    "| no      | yes       |\n",
    "| yes     | no        |\n",
    "| no      | yes       |\n",
    "| yes     | yes       |\n",
    "| yes     | yes       |\n",
    "\n",
    "-   create two vectors called `actuals` and `predicted` with the data\n",
    "    above.\n",
    "-   create a dataframe called `df` that has these columns\n",
    "-   print out the dataframe with `print(df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals   <- CODE \n",
    "predicted   <- CODE \n",
    "df<- data.frame(actuals, predicted)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals<-c('yes','yes','yes','yes','no','no','no','yes','no','yes', 'yes')\n",
    "predicted<-c('yes','no','yes','yes','yes','no','yes','no','yes','yes', 'yes')\n",
    "df<- data.frame(actuals, predicted)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 Accuracy\n",
    "\n",
    "-   Find the accuracy by using `accuracy(actuals, predicted)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is ???\n",
    "\n",
    "### Model 1 Confusion Matrix\n",
    "\n",
    "Remember:\n",
    "\n",
    "|            |            | **Predicted** |            |\n",
    "|------------|------------|---------------|------------|\n",
    "|            |            | *Negative*    | *Positive* |\n",
    "| **Actual** | *Negative* | TN            | FP         |\n",
    "|            | *Positive* | FN            | TP         |\n",
    "\n",
    "-   Find the confusion matrix using `table(actuals, predicted)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   $TP$ is ???\n",
    "-   $FP$ is ???\n",
    "-   $TN$ is ???\n",
    "-   $FN$ is ???\n",
    "\n",
    "### Model 2\n",
    "\n",
    "#### Model 2 predictions vs actuals\n",
    "\n",
    "| actuals | predicted |\n",
    "|---------|-----------|\n",
    "| yes     | no        |\n",
    "| yes     | no        |\n",
    "| yes     | yes       |\n",
    "| yes     | no        |\n",
    "| no      | no        |\n",
    "| no      | yes       |\n",
    "| no      | no        |\n",
    "| yes     | yes       |\n",
    "| no      | no        |\n",
    "| yes     | yes       |\n",
    "| yes     | yes       |\n",
    "\n",
    "-   create two vectors called `actuals` and `predicted` with the data\n",
    "    above.\n",
    "-   create a dataframe called `df` that has these columns\n",
    "-   print out the dataframe with `print(df)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals<-c('yes','yes','yes','yes','no','no','no','yes','no','yes','yes')\n",
    "predicted<-c('no','no','yes','no','no','yes','no','yes','no','yes','yes')\n",
    "df<- data.frame(actuals, predicted)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 Accuracy\n",
    "\n",
    "-   Find the accuracy by using `accuracy(actuals, predicted)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is ???\n",
    "\n",
    "### Model 2 Confusion Matrix\n",
    "\n",
    "Remember:\n",
    "\n",
    "|            |            | **Predicted** |            |\n",
    "|------------|------------|---------------|------------|\n",
    "|            |            | *Negative*    | *Positive* |\n",
    "| **Actual** | *Negative* | TN            | FP         |\n",
    "|            | *Positive* | FN            | TP         |\n",
    "\n",
    "-   Find the confusion matrix using `table(actuals, predicted)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   $TP$ is ???\n",
    "-   $FP$ is ???\n",
    "-   $TN$ is ???\n",
    "-   $FN$ is ???\n",
    "\n",
    "Model 1 or Model 2\n",
    "------------------\n",
    "\n",
    "Which model (Model 1 or Model 2) had the highest accuracy: ???\n",
    "\n",
    "Which model had the most false positives? ???\n",
    "\n",
    "Which model had the most false negatives? ???"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "ir",
   "display_name": "R",
   "language": "R"
  }
 }
}
