---
title: "Measuring Errors for Numerical Predictions"
jupyter:
  kernelspec:
    display_name: R
    language: R
    name: ir
---

```{r echo=T, results='hide', message=F, warning=F}
if (!require("Metrics")) install.packages("Metrics")
library("Metrics")
```

We need to look at ways to measure how far off the predictions from our models are from our actuals. In this document we examine what to do if what we are trying to predict is quantitative (so a number). We will look at classification errors (true/false etc) in another document.     

One reason we need to measure errors is to be able to determine what the best model is in machine learning or predictive analytics. When there are several different models possible, which one is the best?    

This boils down to the one that has the smallest error when run and tested using the testing data set.     

Remember the training data set is used when making the model, but then you test the model using the testing data set. When you test it, you measure how close the predictions are to the actuals using one of the calculations below, depending on the kind of model it is and what you are trying to predict.

## Computing Errors for a Model

Suppose we have this test dataset with predictions based on some model:

| actuals | prediction | 
|---------|------------|
|     11  |     12     |
|     27  |     23     |
|     30  |     32     |
|     13  |     15     |
|     25  |     22     |

Here the actuals and predictions are numbers and we would like to see how far off the actuals are from the prediction.

### Mean Absolute Error (MAE)

Lets find the mean absolute error (MAE) for a prediction and some actuals. This is useful if your prediction is a quantity, like in simple or multiple regression, or for predicting a time series (data where time is the horizontal axis), or some other numerical value:

Lets create a data frame with the actuals and predictions:

```{r}
actuals <- c(11,27,30,13,25)
predictions <- c(12,23,32,15,22)
compare<-data.frame(actuals=actuals,
                    predictions=predictions)
compare
```

We will use the function `mae` from the Metrics package. Actuals go first into this function and then the predictions are second:

```{r}
mae(actuals, predictions)
```

This tells us the average error we make by using our model to predict with.   

Here are the details of this calculation by hand so you can see where it comes from:

```{r}
absError <- abs(predictions - actuals)
info<-data.frame(actuals=actuals,
                 predictions=predictions,
                 absError=absError)
info
```

Now we take the mean of the absolute errors:

```{r}
mean(absError)
```

### Mean Absolute Percentage Error (MAPE)

Next let's find the mean absolute percentage error (MAPE) for a prediction and some actuals:

```{r}
mape(actuals, predictions)
```

```{r}
absError <- abs(predictions - actuals)
percentErr<-absError/actuals
info<-data.frame(actuals=actuals,
                 predictions=predictions,
                 absError=absError,
                 percentErr=round(percentErr,4)
                 )
info
```

Now we take the mean of the percentage errors:

```{r}
mean(percentErr)
```

This tells us on average how far off percentage wise the prediction is from the actual (using the actual as base)

### Root Mean Square Error (RMSE)

This is an error measure that involves averaging the squares of the errors and then taking a square root: 


```{r}
rmse(actuals, predictions)
```

```{r}

deviations<-predictions - actuals
squaredDeviations <- deviations^2
info<-data.frame(actuals=actuals,
                 predictions=predictions,
                 deviations=deviations,
                 squaredDeviations=squaredDeviations)
info
```

Now we take the mean of the percentage errors:

```{r}
sqrt(mean(squaredDeviations))
```

This gives us a typical size of an error or distance away the set of predictions is from the actual. This error has the property of that it penalizes large errors more.

So for example if being off by 10 is more than twice as bad as being off by 5, use the RMSE. If being off by 10 is just twice as bad as being off by 5, then use MAE.

## Comparison to a Different Model

Now suppose we have a new model with these predictions:

| actuals | prediction | 
|---------|------------|
|     11  |     13     |
|     27  |     24     |
|     30  |     31     |
|     13  |     19     |
|     25  |     22     |


How does this compare to the previous model?

```{r}
actuals <- c(11,27,30,13,25)
predictions <- c(13,24,31,19,22)
mae(actuals,predictions)
mape(actuals,predictions)
rmse(actuals,predictions)
```

This model does not do as good a job as our previous model. All three errors are bigger than those for the first model.

So we would conclude that this model is less accurate than the previous one, at least on this test set.

