---
title: "Errors for Numerical Predictions"
jupyter:
  kernelspec:
    display_name: R
    language: R
    name: ir
output:
    ipynbdocument::ipynb_document
---

```{r echo=T, results='hide', message=F, warning=F}
if (!require("Metrics")) install.packages("Metrics")
library("Metrics")
```
Suppose we have the following test data actuals (these are the results for the test data): 

| actual |
|---------|
|     11  |
|     27  |
|     30  |
|     13  |
|     25  |

Suppose our first model machine learning model (called model1) made the following predictions:

| actual | prediction1 | 
|---------|------------|
|     11  |     12     |
|     27  |     23     |
|     30  |     32     |
|     13  |     15     |
|     25  |     22     |

Suppose our second model machine learning model (called model2) made the following predictions:

| actual  | prediction2| 
|---------|------------|
|     11  |     15     |
|     27  |     22     |
|     30  |     29     |
|     13  |     14     |
|     25  |     19     |

Suppose our third model machine learning model (called model3) made the following predictions:

| actual | prediction3| 
|---------|------------|
|     11  |     9      |
|     27  |     25     |
|     30  |     32     |
|     13  |     11     |
|     25  |     20     |

Lets look at the how the predictions do for each model:

First lets do model1

```{r}
actuals <- c(11,27,30,13,25)
predictions1 <- c(12,23,32,15,22)
mae(actuals, predictions1)
mape(actuals, predictions1)
rmse(actuals, predictions1)
```

First lets do model2

```{r}
actuals <- c(11,27,30,13,25)
predictions2 <- c(15,22,29,14,19)
mae(actuals, predictions2)
mape(actuals, predictions2)
rmse(actuals, predictions2)
```

Lets look at model3

```{r}
actuals <- c(11,27,30,13,25)
predictions3 <- c(9,25,32,11,20)
mae(actuals, predictions3)
mape(actuals, predictions3)
rmse(actuals, predictions3)
```

Which model was better? 
