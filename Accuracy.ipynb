{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (!require(\"Metrics\")) install.packages(\"Metrics\")\n",
    "library(\"Metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we had a machine learning model, which was predicting a\n",
    "categorical result like `yes` or `no` based on some predictors.\n",
    "\n",
    "Here is an example where we are predicting `Purchased` based on `Salary`\n",
    "and `Age`. Suppose we started with this **actual** data:\n",
    "\n",
    "#### Actual Observed Data\n",
    "\n",
    "| Salary | Age | Purchased |\n",
    "|--------|-----|-----------|\n",
    "| 53900  | 45  | yes       |\n",
    "| 50000  | 32  | no        |\n",
    "| 55900  | 57  | yes       |\n",
    "| 55600  | 29  | yes       |\n",
    "\n",
    "This means we know for sure that there was someone with `53900` salary\n",
    "and who was `45` and they did purchase whatever we are interested in\n",
    "here.\n",
    "\n",
    "Suppose we came up with a model, that made predictions of this data.\n",
    "\n",
    "And suppose this model made predictions like these:\n",
    "\n",
    "#### Model Predictions\n",
    "\n",
    "| Salary | Age | Prediction |\n",
    "|--------|-----|------------|\n",
    "| 53900  | 45  | no         |\n",
    "| 50000  | 32  | no         |\n",
    "| 55900  | 57  | yes        |\n",
    "| 55600  | 29  | yes        |\n",
    "\n",
    "We are interested how accurate our model is on this data.\n",
    "\n",
    "So for our predictions, how many of them did we get wrong?\n",
    "\n",
    "Accuracy of Predictions\n",
    "-----------------------\n",
    "\n",
    "Suppose we have some actuals and predicted to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals   <- c(\"yes\", \"no\", \"yes\", \"yes\")\n",
    "predicted   <- c(\"no\", \"no\", \"yes\", \"yes\")\n",
    "df<- data.frame(actuals, predicted)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Accuracy\n",
    "\n",
    "We can find the accuracy from this table as follows:\n",
    "\n",
    "We calculate the proportion of agreement. This is called the\n",
    "**accuracy** of the model. The formula is just this:\n",
    "\n",
    "$$accuracy = \\frac{\\text{number of correct predictions}}{\\text{number of all predictions}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Terminology:\n",
    "\n",
    "The prediction is called **positive** or **negative**:\n",
    "\n",
    "-   When the **prediction** is **yes** that is called a **positive**.\n",
    "-   When the **prediction** is **no** that is called a **negative**.\n",
    "\n",
    "The prediction is correct or incorrect:\n",
    "\n",
    "-   **true** means the prediction was correct\n",
    "-   **false** means the prediction was incorrect\n",
    "\n",
    "| Prediction Correct? | Prediction           |\n",
    "|---------------------|----------------------|\n",
    "| True or False       | Positive or Negative |\n",
    "\n",
    "So we have *true positive*, *false positive*, *true negative*, and\n",
    "*false negative*\n",
    "\n",
    "-   $TP$ prediction was yes, actual was yes\n",
    "-   $FP$ prediction was yes, actual was no\n",
    "-   $TN$ prediction was no, actual was no\n",
    "-   $FN$ prediction was no, actual was yes\n",
    "\n",
    "We can print out the confusion matrix like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix\n",
    "----------------\n",
    "\n",
    "|            |            | **Predicted** |            |\n",
    "|------------|------------|---------------|------------|\n",
    "|            |            | *Negative*    | *Positive* |\n",
    "| **Actual** | *Negative* | TN            | FP         |\n",
    "|            | *Positive* | FN            | TP         |\n",
    "\n",
    "Here are the results from the above:\n",
    "\n",
    "-   $TP$ prediction yes, actual yes - ??? times\n",
    "-   $FP$ prediction yes, actual no - ??? times\n",
    "-   $TN$ prediction no, actual no - ??? time\n",
    "-   $FN$ prediction no, actual yes - ??? time\n",
    "\n",
    "Model 2\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals   <- c(\"yes\", \"no\", \"yes\", \"yes\")\n",
    "predicted   <- c(\"yes\", \"yes\", \"yes\", \"no\")\n",
    "df<- data.frame(actuals, predicted)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(actuals, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   $TP$ prediction yes, actual yes - ??? times\n",
    "-   $FP$ prediction yes, actual no - ??? times\n",
    "-   $TN$ prediction no, actual no - ??? time\n",
    "-   $FN$ prediction no, actual yes - ??? time\n",
    "\n",
    "Finally we can write the accuracy in terms of these:\n",
    "\n",
    "$$\n",
    "accuracy = \\frac{\\text{number of correct predictions}}{\\text{number of all predictions}} = \\frac{TP+TN}{TP+TN+FP+FN}\n",
    "$$\n",
    "\n",
    "### Similarities with Jury Trials and Hypothesis Testing\n",
    "\n",
    "|            |            | **Predicted** |            |\n",
    "|------------|------------|---------------|------------|\n",
    "|            |            | *Negative*    | *Positive* |\n",
    "| **Actual** | *Negative* | TN            | FP         |\n",
    "|            | *Positive* | FN            | TP         |\n",
    "\n",
    "Analogies:\n",
    "\n",
    "|            |            | **Jury**    |                 |\n",
    "|------------|------------|-------------|-----------------|\n",
    "|            |            | *Innocent*  | *Guilty*        |\n",
    "| **Actual** | *Innocent* | ok          | innocent jailed |\n",
    "|            | *Guilty*   | guilty free | ok              |\n",
    "\n",
    "|            |            | **Hyp Test of Null** |          |\n",
    "|------------|------------|----------------------|----------|\n",
    "|            |            | *accept*             | *reject* |\n",
    "| **Actual** | *true*     | ok                   | type 1   |\n",
    "|            | *not true* | type 2               | ok       |\n",
    "\n",
    "|            |       | **Pregnancy Test** |            |\n",
    "|------------|-------|--------------------|------------|\n",
    "|            |       | *no*               | *yes*      |\n",
    "| **Actual** | *no * | test-, no          | test+, no  |\n",
    "|            | *yes* | test-, yes         | test+, yes |"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "ir",
   "display_name": "R",
   "language": "R"
  }
 }
}
