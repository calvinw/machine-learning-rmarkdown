---
title: "Confusion Matrix"
jupyter:
  kernelspec:
    display_name: R
    language: R
    name: ir
output:
    ipynbdocument::ipynb_document
---

```{r echo=T, results='hide', message=F, warning=F}
if (!require("Metrics")) install.packages("Metrics")
library("Metrics")
```

Suppose we had a machine learning model, which was predicting a category like `yes` or `no` based on some predictors. For example the data might be this:     

```{r}
actuals   <- factor(c(T,T,T,F,F,F,T,F,T,F,F,F), levels=c(T,F))
predicted <- factor(c(T,T,T,F,T,T,F,T,T,F,T,F), levels=c(T,F))
df<- data.frame(predicted, actuals)
df
```

So to summarize:

-  PUT ANSWER HERE  were predicted positive and they were actually positive 
-  PUT ANSWER HERE  were predicted negative and they were actually negative 
-  PUT ANSWER HERE  were predicted positive and they were actually negative 
-  PUT ANSWER HERE  were predicted negative and they were actually positive 

We can represent that in a table called a "confusion matrix" as follows:   

| 		     | Actual Positive     | Actual Negative       |
|--------------------|----------------|----------------------------|
|  **Predicted Positive** |  PUT ANS HERE  | PUT ANS HERE          |
|  **Predicted Negative**  |  PUT ANS HERE  | PUT ANS HERE         |

We can find the accuracy from this table as follows:

```{r}
accuracy(predicted, actuals)
```

We can print out the confusion matrix like this:

```{r}
table(df)
```

