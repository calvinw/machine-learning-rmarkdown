---
title: "Multicollinearity in Ice Cream Sales"
jupyter:
  kernelspec:
    display_name: R
    language: R
    name: ir
output:
    ipynbdocument::ipynb_document
---

```{r}
options("scipen"=100, "digits"=4)
```

Suppose we have some data for gallons of ice cream sold and the temperatures on two different scales. Does this affect the regression model? Yes, since there is redundancy in the fahrenheit and celsius predictors that leads to what is called "collinearity". It may mean that the p-values for one or more of the predictors is large even when there is some correlation between the predictor and the outcome.   

Usually one of the predictors needs to be excluded from the model to fix this. You can search for this kind of thing by looking at the correlation matrix in R.   

Here is the original data:   

```{r}
fahrenheit <- c(73,65,81,90,75,77,82,93,86,79)
celsius <- c(22,18,27,32,23,25,27,33,30,26)
gallons <- c(110,95,135,160,97,105,120,175,140,121)
```

Let's create a data frame:

```{r}
df <- data.frame(fahrenheit, celsius, gallons)
df
```

```{r}
cor(df)
```

```{r}
plot(df, upper.panel=NULL)
```

```{r}
model = lm(gallons~fahrenheit+celsius, data=df)
summary(model)
```

We see that both p-values are two big. We remove the predictor that has the highest p-value, which is `celsius`. 


```{r}
modelFahrenheight = lm(gallons~fahrenheit, data=df)
summary(modelFahrenheight)
```

We see now that `fahrenheit` is significant and the model looks good for one predictor. 

Probably the model with just `celsius` would work as well. Just for comparison lets  run the model with just `celsius`: 

```{r}
modelCelsius = lm(gallons~celsius, data=df)
summary(modelCelsius)
```
