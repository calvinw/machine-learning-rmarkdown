---
title: "Purchased Knn"
jupyter:
  kernelspec:
    display_name: R
    language: R
    name: ir
---

```{r echo=T, results='hide', message=F, warning=F}
options(repr.plot.width=4, repr.plot.height=4)
options("scipen"=100, "digits"=4)
if(!require("readr")) install.packages("readr")
if(!require("knitr")) install.packages("knitr")
if(!require("Metrics")) install.packages("Metrics")
if(!require("class")) install.packages("class")
library("readr")
library("Metrics")
library("class")
library("knitr")
knitr::opts_chunk$set(fig.width=4, fig.height=4)
normalize <- function(x) {
    return ( (x-min(x))/(max(x)-min(x)) )
}
```

## Using Knn to predict will Purchase

So here is the data we have, this will be our training data:

- `Purchased` is our result or outcome
- `Salary`, `Age` are the predictors

This data is about what kinds of customers responded to an email ad offering a special deal on a product. Were they high salary or low salary and what about their age? We would like to know if `Salary` and `Age` of the customer can predict whether they bought an item based on the offer they received.

Here is our training set:

| Salary | Age | Purchased |
|--------|-----|-----------|
| 53700  | 41  | no        |
| 65300  | 37  | no        |
| 48900  | 45  | yes       |
| 64800  | 49  | yes       |
| 69200  | 30  | yes       |
| 55900  | 57  | yes       |
| 48600  | 26  | no        |
| 72000  | 60  | yes       |
| 45300  | 34  | no        |
| 69000  | 32  | yes       |
| 73200  | 52  | yes       |

Lets read it in:

```{r}
trainurl<- "https://docs.google.com/spreadsheets/d/e/2PACX-1vTx21HIv4xKL2c94OLQzMiNPmv5YXw-vAy3dRBL0gnknSMKMK4Ur_4nYGGyDtvS20iU3_r-_zZfQQmj/pub?gid=0&single=true&output=csv"
traindf<-read.csv(trainurl, stringsAsFactors=TRUE)
str(traindf)
```

Lets print it out to make sure it looks okay:

```{r}
print(traindf)
```

Lets take a look at a graph of the two predictors:

```{r}
plot(traindf$Salary, traindf$Age, col=traindf$Purchased, 
     pch=19,
     xlab="Salary",
     ylab="Age",
     xlim=c(40000,80000), ylim=c(20,70))
```

Red dots represent "yes" and black dots represent "no"

We are going to model this situation using Knn (K nearest neighbors):

Knn needs both the training and the test set passed to it:

Here is our test set:

| Salary | Age | Purchased |
|--------|-----|-----------|
| 53900  | 45  | yes       |
| 64800  | 49  | yes       |
| 50000  | 32  | no        |
| 55900  | 57  | yes       |
| 55600  | 29  | no        |
| 60000  | 25  | no        |

Lets read it in:

```{r}
#read the testing set
testurl<- "https://docs.google.com/spreadsheets/d/e/2PACX-1vTx21HIv4xKL2c94OLQzMiNPmv5YXw-vAy3dRBL0gnknSMKMK4Ur_4nYGGyDtvS20iU3_r-_zZfQQmj/pub?gid=572053114&single=true&output=csv"
testdf<-read.csv(testurl, stringsAsFactors=TRUE)
str(testdf)
```

Lets print it out:

```{r}
print(testdf)
```

Now lets extract the last column of the training dataset since we need it as the 'cl' argument in the knn function. We also remove the `Purchased` column from both the training and the testing data frames since knn expects these both to have no result column. 

Before we remove it from each dataframe though we save it in a vector:   

```{r}
train_target <- traindf$Purchased 
traindf$Purchased <- NULL

test_target <- testdf$Purchased
testdf$Purchased <- NULL

print(traindf)
```

Now before we run the Knn algorithm, we need to normalize (or rescale) the predictors.  The following code normalizes both the predictor columns in the each of our dataframes:

```{r}
traindf_n <- as.data.frame(lapply(traindf, normalize))
testdf_n <- as.data.frame(lapply(testdf, normalize))
print(traindf_n)
```

Notice how the predictors are all normalized into the range 0.0 to 1.0

Next we call the knn method, which creates the model using the training data, then does predictions for the testing data, and returns the predictions all at once. You must also pass a k value to tell the method how many "nearest neighbors" to use for the model.   

```{r}
set.seed(1234)
prediction<-knn(traindf_n, testdf_n, cl=train_target, k=3)
print(prediction)
```

We put the actuals and predictions back into the test data frame so we can check accuracy:

```{r}
testdf$Purchased <- test_target
testdf$Prediction <- prediction
print(testdf)
```

Here is the confusion matrix and the accuracy:

```{r}
table(prediction=prediction,actual=test_target)
accuracy(test_target, prediction)
```

Lets look at some plots of the test data values.    

First we normalize the predictors for those test values:

```{r}
normSalary<-normalize(testdf_n$Salary)
normAge<-normalize(testdf_n$Age)
```
Now lets examine the 1st data value in the test set:

```{r}
plot(traindf_n$Salary, traindf_n$Age, col=train_target, pch=19, xlab="Salary", ylab="Age", xaxt='n', yaxt='n') 
points(normSalary[1], normAge[1], pch=19, col="blue")
print(testdf[1,])
```
Here is the 2nd data value in the testing set:

```{r}
plot(traindf_n$Salary, traindf_n$Age, col=train_target, pch=19, xlab="Salary", ylab="Age", xaxt='n', yaxt='n') 
points(normSalary[2], normAge[2], pch=19, col="blue")
print(testdf[2,])
```
Here is the 3nd data value in the testing set:

```{r}
plot(traindf_n$Salary, traindf_n$Age, col=train_target, pch=19, xlab="Salary", ylab="Age", xaxt='n', yaxt='n') 
points(normSalary[3], normAge[3], pch=19, col="blue")
print(testdf[3,])
```
Here is the 4th data value in the testing set:

```{r}
plot(traindf_n$Salary, traindf_n$Age, col=train_target, pch=19, xlab="Salary", ylab="Age", xaxt='n', yaxt='n') 
points(normSalary[4], normAge[4], pch=19, col="blue")
print(testdf[4,])
```
Here is the 5th data value in the testing set:

```{r}
plot(traindf_n$Salary, traindf_n$Age, col=train_target, pch=19, xlab="Salary", ylab="Age", xaxt='n', yaxt='n') 
points(normSalary[5], normAge[5], pch=19, col="blue")
print(testdf[5,])
```

Here is the 6th data value in the testing set:

```{r}
plot(traindf_n$Salary, traindf_n$Age, col=train_target, pch=19, xlab="Salary", ylab="Age", xaxt='n', yaxt='n') 
points(normSalary[6], normAge[6], pch=19, col="blue")
print(testdf[6,])
```

