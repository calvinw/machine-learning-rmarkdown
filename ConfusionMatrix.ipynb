{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (!require(\"Metrics\")) install.packages(\"Metrics\")\n",
    "library(\"Metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions and Actuals\n",
    "\n",
    "Suppose we had a machine learning model, which was predicting a category\n",
    "like `yes` or `no` based on some predictors. For example the data might\n",
    "be this:\n",
    "\n",
    "-   5 were predicted yes and they were actually yes\n",
    "-   4 were predicted no and they were actually no\n",
    "-   2 were predicted yes and they were actually no\n",
    "-   1 were predicted no and they were actually yes\n",
    "\n",
    "We can represent that in a table called a “confusion matrix” as follows:\n",
    "\n",
    "|                   | Actual Yes | Actual No |\n",
    "|-------------------|:-----------|:----------|\n",
    "| **Predicted Yes** | 5          | 2         |\n",
    "| **Predicted No**  | 1          | 4         |\n",
    "\n",
    "The different parts of this table have the following terms that go with\n",
    "them:\n",
    "\n",
    "|                   | Actual Yes     | Actual No      |\n",
    "|-------------------|:---------------|:---------------|\n",
    "| **Predicted Yes** | True Positive  | False Positive |\n",
    "| **Predicted No**  | False Negative | True Negative  |\n",
    "\n",
    "-   **True Positive** – Predicted *positive* and was actually *positive*\n",
    "-   **True Negative** – Predicted *negative* and was actually *negative*\n",
    "-   **False Positive** – Predicted *positive* and was actually\n",
    "    *negative*\n",
    "-   **False Negative** – Predicted *negative* and was actually\n",
    "    *positive*\n",
    "\n",
    "Here are some abbreviations for these:\n",
    "\n",
    "|                   | Actual Yes | Actual No |\n",
    "|-------------------|:-----------|:----------|\n",
    "| **Predicted Yes** | TP         | FP        |\n",
    "| **Predicted No**  | FN         | TN        |\n",
    "\n",
    "With this notation here are some things that help keep the\n",
    "interpretations clear:\n",
    "\n",
    "-   T represents correctly classified\n",
    "-   F represents incorrectly classified\n",
    "-   P means it was predicted positive\n",
    "-   N means it was predicted negative\n",
    "-   TP and TN are correctly classified\n",
    "-   FN and FP are misclassified\n",
    "\n",
    "### Accuracy\n",
    "\n",
    "Accuracy is the idea of the proportion of correct predictions out of all\n",
    "predictions:\n",
    "\n",
    "We can find the accuracy from this table as follows:\n",
    "\n",
    "$$ accuracy = (TP+TN)/(TP+TN+FP+FN) $$\n",
    "\n",
    "Lets do the calculations with R.\n",
    "\n",
    "First lets print out the actuals and the predicted as dataframe just to\n",
    "see what we havee:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals   <- factor(c(T,F,T,F,T,T,T,F,T,F,T,F), levels=c(T,F))\n",
    "predicted <- factor(c(T,T,T,F,F,T,F,F,T,F,T,F), levels=c(T,F))\n",
    "df<- data.frame(predicted, actuals)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compute the accuracy as follows using the `accuracy`\n",
    "function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(predicted, actuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compute the confusion matrix as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying TP, TN, FP, and FN\n",
    "\n",
    "Now lets try an example and see if we can identify “true positives”,\n",
    "“true negatives”, “false positives” and “false negatives”.\n",
    "\n",
    "Picture 165 people standing in a line. Each person walks up to you, and\n",
    "you either predict “you have the disease” or “you don’t have the\n",
    "disease”. In reality, each person either has the disease or does not\n",
    "have the disease.\n",
    "\n",
    "Identify each of the following situations:\n",
    "\n",
    "-   If you tell someone “you have the disease” and they don’t have it.\n",
    "    That’s what happened with 10 of the people.\n",
    "-   If you tell someone “you don’t have the disease” and they don’t have\n",
    "    it. That’s what happened with 50 people.\n",
    "-   If you tell someone “you don’t have the disease” and they do have\n",
    "    it. That’s what happened with 5 people.\n",
    "-   If you tell someone “you have the disease” and they do have the\n",
    "    disease. That’s what happened with 100 of the people.\n",
    "\n",
    "Fill in the following:\n",
    "\n",
    "|                         | Has Disease | Doesnt Have |\n",
    "|-------------------------|:------------|:------------|\n",
    "| **Predict Has Disease** | ???         | ???         |\n",
    "| **Predict Doesnt Have** | ???         | ???         |\n",
    "\n",
    "### Sensitivity (True Positive Rate)\n",
    "\n",
    "If something is actually positive, how good is the classifier at\n",
    "predicting yes?\n",
    "\n",
    "$$ sensitivity = TP/(FN+TP) $$\n",
    "\n",
    "Example of when you want high sensitivity (want FN kept to minimum):\n",
    "classifying disease\n",
    "\n",
    "-   TP Predict has disease and actually has disease\n",
    "-   FN Predict doesnt have disease but actually has disease (this is\n",
    "    bad)\n",
    "\n",
    "Here is the confusion matix for this example:\n",
    "\n",
    "|                         | Has Disease | Doesnt     |\n",
    "|-------------------------|:------------|:-----------|\n",
    "| **Predict Has Disease** |             | Not so bad |\n",
    "| **Predict Doesn’t**     | This is bad |            |\n",
    "\n",
    "Note in this case: FP means predicted to have disease but actually\n",
    "doesnt\n",
    "\n",
    "### Specificity (True Negative Rate)\n",
    "\n",
    "If something is actually negative, how good is the classifier at\n",
    "predicting no?\n",
    "\n",
    "$$ specificity = TN/(FP+TN) $$\n",
    "\n",
    "Example of when you want high specifity (want FP kept to minimum): spam\n",
    "filter\n",
    "\n",
    "-   TN Predicted Not Spam and actually not Spam (goes to inbox)\n",
    "-   FP Predicted Spam but Not Spam (goes in Spam box …this is bad)\n",
    "\n",
    "Here is the confusion matix for this example:\n",
    "\n",
    "|                    | Spam       | Not Spam    |\n",
    "|--------------------|:-----------|:------------|\n",
    "| **Predicted Spam** |            | This is bad |\n",
    "| **Predicted Not**  | Not so bad |             |\n",
    "\n",
    "Note in this case: FN means predicted not spam but actually is spam"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "name": "ir",
   "display_name": "R",
   "language": "R"
  }
 }
}

