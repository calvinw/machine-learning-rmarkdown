---
title: "Accuracy"
jupyter:
  kernelspec:
    display_name: R
    language: R
    name: ir
---

```{r echo=T, results='hide', message=F, warning=F}
if (!require("Metrics")) install.packages("Metrics")
library("Metrics")
```

Suppose we had a machine learning model, which was predicting a categorical result like `yes` or `no` based on some predictors. 

Here is an example where we are predicting `Purchased` based on `Salary` and `Age`. Suppose we started with this **actual** data: 

#### Actual Observed Data

| Salary | Age | Purchased |
|-|-|-|
| 53900 | 45 | yes |
| 50000 | 32 | no |
| 55900 | 57 | yes |
| 55600 | 29 | yes |


This means we know for sure that there was someone with `53900` salary and who was `45` and they did purchase whatever we are interested in here. 

Suppose we came up with a model, that made predictions of this data.  

And suppose this model made predictions like these:

#### Model Predictions 

| Salary | Age | Prediction |
|-|-|-|
| 53900 | 45 | no |
| 50000 | 32 | no |
| 55900 | 57 | yes |
| 55600 | 29 | yes |

We are interested how accurate our model is on this data. 

So for our predictions, how many of them did we get wrong?

## Accuracy of Predictions 

Suppose we have some actuals and predicted to compare.

```{r}
actuals   <- c("yes", "no", "yes", "yes")
predicted   <- c("no", "no", "yes", "yes")
df<- data.frame(actuals, predicted)
print(df)
```

### Calculating Accuracy

We can find the accuracy from this table as follows:

We calculate the proportion of agreement. This is called the **accuracy** of the model. The formula is just this: 

$$accuracy = \frac{\text{number of correct predictions}}{\text{number of all predictions}}$$


```{r}
accuracy(actuals, predicted)
```

### Confusion Matrix 

Terminology:

The prediction is called **positive** or **negative**:

- When the **prediction** is **yes** that is called a **positive**.
- When the **prediction** is **no** that is called a **negative**.

The prediction is correct or incorrect:

- **true** means the prediction was correct
- **false** means the prediction was incorrect


| Prediction Correct? | Prediction | 
|--------|--------|
| True or False | Positive or Negative  |

So we have _true positive_, _false positive_, _true negative_, and _false negative_

- $TP$ prediction was yes, actual was yes 
- $FP$ prediction was yes, actual was no 
- $TN$ prediction was no, actual was no 
- $FN$ prediction was no, actual was yes 

We can print out the confusion matrix like this:

```{r}
table(actuals, predicted)
```

## Confusion Matrix 
|           |            |**Predicted**|            |
|-----------|------------|-------------|------------|
|           |            | _Negative_  | _Positive_ |
|**Actual** | _Negative_ |     TN      |     FP     |
|           | _Positive_ |     FN      |     TP     |


Here are the results from the above:

- $TP$ prediction yes, actual yes - ??? times 
- $FP$ prediction yes, actual no - ??? times 
- $TN$ prediction no, actual no - ??? time 
- $FN$ prediction no, actual yes - ??? time 

## Model 2 

```{r}
actuals   <- c("yes", "no", "yes", "yes")
predicted   <- c("yes", "yes", "yes", "no")
df<- data.frame(actuals, predicted)
df
```

### Calculating Accuracy

```{r}
accuracy(actuals, predicted)
```

### Confusion Matrix 

```{r}
table(actuals, predicted)
```

- $TP$ prediction yes, actual yes - ??? times 
- $FP$ prediction yes, actual no - ??? times 
- $TN$ prediction no, actual no - ??? time 
- $FN$ prediction no, actual yes - ??? time 

Finally we can write the accuracy in terms of these:

$$
accuracy = \frac{\text{number of correct predictions}}{\text{number of all predictions}} = \frac{TP+TN}{TP+TN+FP+FN}
$$


### Similarities with Jury Trials and Hypothesis Testing

|           |            |**Predicted**|            |
|-----------|------------|-------------|------------|
|           |            | _Negative_  | _Positive_ |
|**Actual** | _Negative_ |     TN      |     FP     |
|           | _Positive_ |     FN      |     TP     |


Analogies: 

|           |            |  **Jury**   |            |
|-----------|------------|-------------|------------|
|           |            | _Innocent_  | _Guilty_   |
|**Actual** | _Innocent_ |     ok      | innocent jailed |
|           | _Guilty_   |  guilty free|     ok     |


|           |            |**Hyp Test of Null**|     |
|-----------|------------|-------------|------------|
|           |            |  _accept_   | _reject_   |
|**Actual** | _true_     |     ok      |   type 1   |
|           | _not true_ |     type 2  |     ok     |

|           |            |**Pregnancy Test**|       |
|-----------|------------|-------------|------------|
|           |            |    _no_     |   _yes_    |
|**Actual** | _no_       | test-, no   | test+, no  |
|           | _yes_      | test-, yes  | test+, yes |


