---
title: "Accuracy"
jupyter:
  kernelspec:
    display_name: R
    language: R
    name: ir
---

```{r echo=T, results='hide', message=F, warning=F}
if (!require("Metrics")) install.packages("Metrics")
library("Metrics")
```

Suppose we had a machine learning model, which was predicting a result like `yes` or `no` based on some predictors. 

Here is an example of the actual data:

| Salary | Age | Actual |
|-|-|-|
| 53900 | 45 | yes |
| 50000 | 32 | no |
| 55900 | 57 | yes |
| 55600 | 29 | yes |

And here is the example of our predictions:

| Salary | Age | Prediction |
|-|-|-|
| 53900 | 45 | no |
| 50000 | 32 | no |
| 55900 | 57 | yes |
| 55600 | 29 | no  |

We are interested how accurate our predictions are. 

So for our predictions, how many of them did we get wrong?

## Finding the Accuracy of our Predictions 

```{r}
actuals   <- c("yes", "no", "yes", "yes")
predicted   <- c("no", "no", "yes", "no")
df<- data.frame(actuals, predicted)
df
```

### Calculating Accuracy

We can find the accuracy from this table as follows:

We calculate the proportion of agreement. This is called the **accuracy** of the model. The formula is just this: 

$$accuracy = \frac{\text{number of correct predictions}}{\text{number of all predictions}}$$


```{r}
accuracy(actuals, predicted)
```

### Confusion Matrix 

Terminology:

The prediction is called **positive** or **negative**:

- When the **prediction** is **yes** that is called a **positive**.
- When the **prediction** is **no** that is called a **negative**.

The prediction is correct or incorrect:

- **true** means the prediction was correct
- **false** means the prediction was incorrect


| Prediction Correct? | Prediction | 
|--------|--------|
| True or False | Positive or Negative  |

So we have _true positive_, _false positive_, _true negative_, and _false negative_

- $TP$ prediction was yes, actual was yes 
- $FP$ prediction was yes, actual was no 
- $TN$ prediction was no, actual was no 
- $FN$ prediction was no, actual was yes 

We can print out the confusion matrix like this:

```{r}
table(actuals, predicted)
```

## Confusion Matrix 
|           |            |**Predicted**|            |
|-----------|------------|-------------|------------|
|           |            | _Negative_  | _Positive_ |
|**Actual** | _Negative_ |     TN      |     FP     |
|           | _Positive_ |     FN      |     TP     |

Here are the results from the above:

- $TP$ prediction yes, actual yes - 1 times 
- $FP$ prediction yes, actual no - 0 times 
- $TN$ prediction no, actual no - 1 time 
- $FN$ prediction no, actual yes - 2 time 

## Model 2 

```{r}
actuals   <- c("no", "no", "yes", "yes", "no", "no", "yes")
predicted   <- c("yes", "yes", "yes", "no", "yes", "yes", "yes")
df<- data.frame(actuals, predicted)
df
```

### Calculating Accuracy

```{r}
accuracy(actuals, predicted)
```

### Confusion Matrix 

```{r}
table(actuals, predicted)
```

- $TP$ prediction yes, actual yes - 1 times 
- $FP$ prediction yes, actual no - 4 times 
- $TN$ prediction no, actual no - 0 time 
- $FN$ prediction no, actual yes - 1 time 

Finally we can write the accuracy in terms of these:

$$
accuracy = \frac{\text{number of correct predictions}}{\text{number of all predictions}} = \frac{TP+TN}{TP+FP+TN+FN}
$$

