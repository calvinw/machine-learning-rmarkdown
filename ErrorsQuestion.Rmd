---
title: "Errors Question"
jupyter:
  kernelspec:
    display_name: R
    language: R
    name: ir
---

```{r echo=T, results='hide', message=F, warning=F}
if (!require("Metrics")) install.packages("Metrics")
library("Metrics")
```

```{r, include=FALSE}
assignment<-T
```

Suppose we have two models, `model1` and `model2` created from some training data set (not shown).

The models can be described as follows `y~x1+x2`. So the result is `y` and the predictors are `x1` and `x2`.

## Test Data 

Now we wish to evaluate the two models using the following test data set:

| x1    |  x2    | y    | 
|-------|--------|------|
| 23    | TRUE  | 83   | 
| 15    | FALSE | 78   | 
| 17    | FALSE | 75   |
| 21    | TRUE | 82   |
| 25    | TRUE | 89   |


## Model1 Predictions 

Suppose our first model (called model1) made the following predictions:

| x1    |  x2    | ypred1 | 
|-------|--------|------|
| 23    | TRUE  | 80   | 
| 15    | FALSE | 72   | 
| 17    | FALSE | 74   |
| 21    | TRUE | 85   |
| 25    | TRUE | 91   |


## Model2 Predictions 

Suppose our second model model (called model2) made the following predictions:

| x1    |  x2    | ypred2 | 
|-------|--------|------|
| 23    | TRUE  | 79   | 
| 15    | FALSE | 78   | 
| 17    | FALSE | 76   |
| 21    | TRUE | 81   |
| 25    | TRUE | 87   |

Lets create some vectors with the actuals and the predictions: 

## Vectors for Actuals and Predictions 

```{r include=assignment, eval=FALSE}
actuals <- CODE
predictions1 <- CODE
predictions2 <- CODE
```
```{r include=!assignment}
actuals <- c(83,78,75,82,89)
predictions1 <- c(80,72,74,85,91)
predictions2 <- c(79,78,76,81,87)
```

## MAE 

### Model 1 

- find the `mae` of the `actuals` and `predictions1` by passing these as arguments to the `mae` function:
```{r include=assignment, eval=FALSE}
mae(CODE)
```
```{r include=!assignment}
mae(actuals, predictions1)
```

### Model 2 

- find the `mae` of the `actuals` and `predictions2` by passing these as arguments to the `mae` function:
```{r include=assignment, eval=FALSE}
mae(CODE)
```
```{r include=!assignment}
mae(actuals, predictions2)
```

#### Results for MAE    

MAE for Model 1: ???   
MAE for Model 2: ???   
Which was better: ???  

## MAPE

### Model 1 

- find the `mape` of the `actuals` and `predictions1` by passing these as arguments to the `mape` function:
```{r include=assignment, eval=FALSE}
mape(CODE)
```
```{r include=!assignment}
mape(actuals, predictions1)
```

### Model 2 

- find the `mape` of the `actuals` and `predictions2` by passing these as arguments to the `mape` function:
```{r include=assignment, eval=FALSE}
mape(CODE)
```
```{r include=!assignment}
mape(actuals, predictions2)
```

#### Results for MAPE    

MAPE for Model 1: ???   
MAPE for Model 2: ???   
Which was better: ???  


## RMSE

### Model 1 

- find the `rmse` of the `actuals` and `predictions1` by passing these as arguments to the `rmse` function:
```{r include=assignment, eval=FALSE}
rmse(CODE)
```
```{r include=!assignment}
rmse(actuals, predictions1)
```

### Model 2 

- find the `rmse` of the `actuals` and `predictions2` by passing these as arguments to the `rmse` function:
```{r include=assignment, eval=FALSE}
rmse(CODE)
```
```{r include=!assignment}
rmse(actuals, predictions2)
```

#### Results for RMSE 

RMSE for Model 1: ???   
RMSE for Model 2: ???   
Which was better: ???  


Which model was better overall: ???
